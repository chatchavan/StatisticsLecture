---
title: "ANOVA example"
output: html_notebook
---
```{r setup}
if (!require("pacman")) install.packages("pacman", repos='https://stat.ethz.ch/CRAN/'); library(pacman)
p_load(ggplot2, 
       DT, 
       plyr,
       dplyr, 
       tidyr,
       assertthat,
       car,
       GetoptLong,
       tibble,
       cowplot,
       readr,
       broom,
       GGally)

# string interpolation
qq.options("code.pattern" = "#\\{CODE\\}") 

# plot theme
myTheme <- theme(panel.background = element_blank(), panel.grid.major = element_line(color="lightgrey", size = 0.2))

# decimal output
options(digits=2)

# datatable
options(DT.options = list(pageLength = 10))
options(DT.autoHideNavigation = TRUE)

# We ensure that the following packages are installed, but left unloaded to 
# prevent conflicts.
# (We can still use the functions inside, e.g., MASS::stepAIC() )

p_load(MASS); p_unload(MASS)     # for stepwise regression 
p_load(multcomp)  # for glht, mcp
```

# Data
This dataset is simulated from the following hypothetical experiment:

> A company has recently invested in smart watches (known as the Mango) for all sales executives. They have tasked your team to evaluate the impact of the Mango on productivity among the sales staff. They believe that having email on one’s wrist will cause faster response times to customers, and therefore more sales. 
> Data will be collected for Sales Team A over a period of 3 months after the introduction of Mangos. Control data will be collected from members of Sales Team B over the same period—this group will not yet be equipped with Mangos. However, they will receive their Mangos in month 4, giving the researchers both between subjects (A compared to B) and within subjects (pre-Mango B compared to post-Mango B) data.

This analysis will focus on the relationship between inattention score (Kushlev et al. (2016)) and the usage time of the first month (in minutes). We hypothesize that users with higher inattention score will use the watch longer.

```{r}
age_df          <- read_csv("../data/Mango Watch/age.csv")
usageTime_df    <- read_csv("../data/Mango Watch/usageTime.csv")
team_df         <- read_csv("../data/Mango Watch/team.csv")
sus_df          <- read_csv("../data/Mango Watch/SUS.csv")
responseTime_df <- read_csv("../data/Mango Watch/responseTime.csv")
inattention_df  <- read_csv("../data/Mango Watch/inattention.csv")

responseTime_df <- left_join(team_df, responseTime_df, by = "ID")

ageUsageTime_df <- left_join(team_df, age_df, by = "ID") %>% 
  left_join(usageTime_df, by = "ID") %>% 
  left_join(inattention_df, by = "ID")

sus_df <- left_join(team_df, sus_df, by = "ID")
```


# Exploratory plot
```{r}
time_inattention <- ageUsageTime_df %>% 
  ggplot(., aes(x = Inattention, y = UsageTime)) +
  geom_point() +
  # geom_smooth(method = "loess", size = 1.5) +
  geom_smooth(method = "lm", size = 1.5, color = "red") +
  xlim(0,4.1)

lm_inattention <- lm(UsageTime ~ Inattention, ageUsageTime_df)
p_TIresid <- ageUsageTime_df %>% 
  ggplot(., aes(x = Inattention, y = lm_inattention$residuals)) +
  geom_point() +
  geom_hline(yintercept = 0, col = "red") +
  xlim(0,4.1)

plot_grid(time_inattention, p_TIresid, ncol = 2)
```

# Linear model
```{r}
lm1 <- lm(UsageTime ~ Inattention, ageUsageTime_df)
summary(lm1)
summ <- summary(lm1)
lm1_t <- tidy(lm1)
lm_ci <- confint(lm1, level = 0.95)
p.value <- pf(summ$fstatistic[1], summ$fstatistic[2], summ$fstatistic[3], lower.tail = FALSE)
```


__Interpretation:__ A simple linear regression was calculated to predict usage time based on inattention score. A statistically significant regression equation was found (F(`r summ$fstatistic[2]`, `r summ$fstatistic[3]`) = `r summ$fstatistic[1]`, p < .001*) with an R^2 = `r summ$r.squared`.

The predicted usage time is equal to `r lm1_t$estimate[1]` + `r lm1_t$estimate[2]` × (inattention score) minutes when the inattention score is measured according to Kushlev et al. (2016).

The usage time increased `r lm1_t$estimate[2]` minutes for each point of the inattention score (95% CI [`r lm_ci["Inattention", "2.5 %"]`, `r lm_ci["Inattention", "97.5 %"]`]).

* exact p-value: `r p.value`


# Analysis of variance (ANOVA)
Comparing to the null model: (there's no relationship between inattention and usage time)
```{r}
anova_r <- anova(lm_inattention)
print(anova_r)
anova_t <- tidy(anova_r)
```

__Interpretation:__ ANOVA indicates that the model `UsageTime ~ Inattention` score fits the data better than the null model `UsageTime ~ 1`, *F*(`r anova_t$df[1]`, `r anova_t$df[2]`) = `r anova_t$statistic[1]`, p < .001 (exact p = `r anova_t$p.value[1]`).

Alternatively, we can specify the null model explicitly. This would yield the same results
```{r}
anova(lm(UsageTime ~ 1, ageUsageTime_df),             # null model (only intercept)
      lm(UsageTime ~ Inattention, ageUsageTime_df)    # intercept and inattention 
)
```


We can add the team into the model and compare them
```{r}
anova_r <-
  anova(lm(UsageTime ~ 1, ageUsageTime_df),                   # null model (only intercept)
        lm(UsageTime ~ Inattention, ageUsageTime_df),         # intercept and inattention 
        lm(UsageTime ~ Inattention + Team, ageUsageTime_df)   # intercept, inattention, and team
  )
print(anova_r)
# alternative way to modify the model
# lm_inattention_team <- update(lm_inattention, . ~ Team ) # adds a term to the right hand side
```

__Interpretation:__ Adding team as an additonal factor yielded a better fit compared to the model with inattention alone as shown in the following table:

```{r}
anova_t <- 
  tidy(anova_r) %>% 
  mutate(F = statistic) %>% 
  select(-rss, -sumsq, -statistic) %>% 
  select(df, res.df, F, p.value)
anova_t
```


# Your turn
Try fitting a linear model between age, usage time, and team
```{r}
# TODO: your code here
# ageUsageTime_df$Age
# ageUsageTime_df$UsageTime
# ageUsageTime_df$Team
```


# Determining regressors
In science, we prefer simpler models. This is a principle known as *Occam's razor*. 
F-statistics represents how good the model fits the data, but it doesn't penalize
complex model. Instead, we use *Akaike information criterion (AIC)* (read "a-kai-kay"; Japanese). AIC accounts for both the fit and the number of parameters in the model.
**We prefer model with minimum AIC.**

## How to select variables to be regressors?

In HCI, we usually use linear models to analyze survey responses. The questions in
the survey are usually designed with a theoretical basis. Therefore, we usually 
construct the model by adding factors that are theoretically sound.
```{r}
m1 <- lm(UsageTime ~ 1, ageUsageTime_df)
m2 <- update(m1, . ~ Inattention)
m3 <- update(m1, . ~ Team)
m4 <- update(m1, . ~ Inattention + Team)

AIC_result <- data.frame(rbind(
  extractAIC(m1),
  extractAIC(m2),
  extractAIC(m3),
  extractAIC(m4)))
colnames(AIC_result) <- c("df", "AIC")

print(AIC_result)
```

**Interpretation:** `Inattention` reduces AIC more than `Team`. Thus, `Inattention` should be prioritized more than `Team`.
However, if the model can afford two parameters, both `Inattention` and `Team` lowers AIC, hence should be included.


# Stepwise automatic modeling
**Only for exploratory analysis (when you have no theoretical basis)**, that we automate the model construction. Below, we enter all factors and perform *step-wise backward regression* which gradually remove the terms to minimize AIC. 
```{r}
wholeModel <- lm(UsageTime ~ Inattention + Team, data = ageUsageTime_df)
step_backward_result <- 
  MASS::stepAIC(wholeModel, 
    direction = "backward", 
    trace = TRUE           # this shows the fit trace so we can track AIC (optional)
    )
```

**Interpretation:** Removing either `Team` or `Inattention` doesn't reduce AIC. Therefore, both terms should still be included in the model.

```{r}
# stepAIC output a model, so you can obtain F statistics using typical anova() 
# and t-statistics of each coefficient with summary()
anova(step_backward_result)
summary(step_backward_result)
```


# Your turn
Determine whether ther interaction term should be included in this model or not.
```{r}
# TODO: your code here
```


# Categorical independent variable
So far, we have been analyzing interval/ratio independent variable. We will now turn our focus to categorical independent variable, which is frequently the case for data collected from HCI experiments.


In the code below, we analyze the effect of `Month` of usage to `ResponseTime` in for Team A. (We disregard the order of `Month` for a moment.)
```{r}
responseTime_df %>% 
  filter(Team == "Team A") %>% 
  mutate(Month = factor(Month)) ->
  rt_teamA
```

## Model fitting
Now we fit a linear model. R will code `Month` into two regressors: `Month2` and `Month3`.
(See `Categorical.Rmd` for an explanation of how categorical variables are coded into multiple binary regressors.)
```{r}
m_month <- lm(ResponseTime ~ Month, data = rt_teamA)
m_month_anova <- anova(m_month)
summary(m_month)
```

The estimated value of `(Intercept)` has the same value as the mean of the first month.

```{r}
rt_teamA %>% 
  group_by(Month) %>% 
  summarise(`Group means` = mean(ResponseTime)) %>% 
  mutate(
    `Equivalent model parameters` = c(
      "(Intercept)",
      "(Intercept) + Month2",
      "(Intercept) + Month3"
    ),
    `Calculation result` = c(
      m_month$coefficients["(Intercept)"],
      m_month$coefficients["(Intercept)"] + m_month$coefficients["Month2"],
      m_month$coefficients["(Intercept)"] + m_month$coefficients["Month3"]
    )
  )
```

**Interpretation:** For models with categorical independent variables, we can use 
the result from the linear regression to infer *only* about the model fit (*F*-statistic) and 
the estimates of group means. We won't interpret the *t*-statistics because
this could incrase error probability (more on this later).

In other words, we can only test the following null hyptothesis:

* \\(H_0\\): There's no difference between the model with a single mean for the whole data vs. the model with individual mean for each month.
* \\(H_1\\): There's some difference between the two models.


For this hypothesis, the results suggest that the fit of the model with three means differs from the model with single mean, \\(F(`r m_month_anova$Df[1]`, `r m_month_anova$Df[2]`) = `r m_month_anova[["F value"]][1]`, p < .001  \\).

(Exact p value = `r m_month_anova[["Pr(>F)"]][1]`)

## Estimation

For models with categorical independent variables, we will have to use additional step
to estimate the mean difference among groups. There are two ways to do this. 

### Planned contrasts

If we have a meaningful plan prior to the analysis, we can use *planned contrasts*
method that allow us to test complex combination of means. In the example below,
we tests two hypotheses:

  * \\(H_{0,1}\\): There's no difference between Month 1 vs. Month 2 and 3 combined.
  * \\(H_{1,1}\\): There's some difference between Month 1 vs. Month 2 and 3 combined.

  * \\(H_{0,1}\\): There's no difference between Month 2 vs. 3.
  * \\(H_{1,1}\\): There's some difference between Month 2 vs. 3.


```{r}
month_contrasts <- glht(m_month,   # glht: Generalized Linear Hypothesis Tests
  linfct = mcp(                    # linfct: LINear FunCTion; mcp: Multiple ComParison
    Month = c(
      "`1` - (`2` + `3`) = 0",     # null hypothesis 1 
      "`2` - `3` = 0"              # null hypothesis 2
      
      # NOTE: We use the backquote (`) around the Month's level 1, 2, and 3 
      # to let R know that we meant the levels in our data, not numbers.
      # Compare this with "0" that was not surrounded by backquotes
)))

print(summary(month_contrasts))
```

The results of `glht` function gives us a breakdown test of both hypotheses as well as the estimate of the differences and standard error (of which we can used to calculate 95% confidence interval). We can plot them out below:


```{r}
contrasts_t <- tidy(confint(month_contrasts))
print(contrasts_t)
contrasts_t %>% 
  ggplot(aes(x = paste(lhs, "==", rhs))) +
  geom_hline(yintercept = 0, color = "grey") +     # reference line
  geom_point(aes(y = estimate)) +                  # point estimate of the difference
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = 0) + # 95% CI of the difference
  expand_limits(y = 0) +
  coord_flip() +
  xlab("Hypothesis") +
  ylab("Estimate of the difference")
  
```

From the graph, we can see that the estimate of the difference including their confidence intervals are very far from zero. This suggests that the data are more compatible with both of our alternative hypotheses (i.e. we beleive that there're differences).

For statistical reports, we'd include the estimate of the differences, the 95% confidence interval, and the results of the two t-tests in a table format.


### Pairwise comparison

When we do exploratory data analysis, we usually don't have any comparisons planned. Thus, we tend to compare the difference of all possible pairs of levels. We can do this using a similar function:

```{r}
month_pairwise <- glht(m_month,
  linfct = mcp(                
    Month = "Tukey"    # Tukey's test will compare all pairs of levels and adjust probability accordingly
))

print(summary(month_pairwise))
pairwise_t <- tidy(confint(month_pairwise))
print(pairwise_t)
pairwise_t %>% 
  ggplot(aes(x = paste(lhs, "==", rhs))) +
  geom_hline(yintercept = 0, color = "grey") +     # reference line
  geom_point(aes(y = estimate)) +                  # point estimate of the difference
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = 0) + # 95% CI of the difference
  expand_limits(y = 0) +
  coord_flip() +
  xlab("Hypothesis") +
  ylab("Estimate of the difference")
```

Note that in the pairwise comparison, we can only compare simple hypotheses. In particular, we cannot compare the differences between group of levels.

# References

* Kostadin Kushlev, Jason Proulx, and Elizabeth W. Dunn. 2016. "Silence Your Phones": Smartphone Notifications Increase Inattention and Hyperactivity Symptoms. In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems (CHI '16). ACM, New York, NY, USA, 1011-1020. DOI: https://doi.org/10.1145/2858036.2858359
