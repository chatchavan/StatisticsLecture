---
title: "Analyzing categorical independent variable"
output: html_notebook
---

```{r setup, echo=FALSE}
knitr::opts_chunk$set(echo = FALSE)

# The "pacman" package automatically install missing packages and load them
if (!require("pacman")) install.packages("pacman", repos='https://stat.ethz.ch/CRAN/'); library(pacman)
p_load(
       DT,         # for showing data table with navigation/search controls
       tidyverse,  # collection of the tidyverse packages (this automatically load the following):
       #dplyr,     #   - for data wrangling
       #tibble,    #   - a stricter alternative to data.frame
       #readr,     #   - a stricter alternative to read.csv
       #ggplot2,   #   - for plotting
                   # other packages in tidyverse that are non-core
       stringr,    #   - for string functions
       tidyr,      #   - for data tidying
       broom,      # for cleaing output from models, e.g., lm()
       cowplot,    # adds plot_grid() to put multiple ggplot()'s togeter
       GGally,     # adds ggpairs() which is a smarter scatterplot matrix
       assertthat, # for unit-testing your functions
       car,        # grab bag of useful functions for NHST
       GetoptLong, # string interpolation. See qq() explanation below
       lubridate,  # utility for parsing and performing arithematic on dates 
       forcats,    # utility for working with factor levels
       Hmisc     # for plotting mean and CI in ggplot
       )

# string interpolation
qq.options("code.pattern" = "#\\{CODE\\}") 

# plot theme
myTheme <- theme(panel.background = element_blank(), panel.grid.major = element_line(color="lightgrey", size = 0.2))

# Decimal output
##   NOTE: This option might cause output to be printed with rounding. (default value = 7)
# options(digits=2)   

# datatable
options(DT.options = list(pageLength = 10))
options(DT.autoHideNavigation = TRUE)


p_load_gh("eclarke/ggbeeswarm") # for beeswarm
p_load(rafalib); p_unload(rafalib)  # for imagemat
p_load(multcomp)  # for glht (contrast and multiple comparison)
```

```{r}
data <- tibble(
  Group = as.factor(rep(c("A", "B", "C"), each=8)), 
  Value = c(1, 2, 4, 1, 1, 2, 2, 3, 3, 4, 4,
           2, 3, 4, 4, 3, 4, 5, 3, 5, 5, 3, 4, 6))
```

Plotting the  data out
```{r}
data %>% 
  ggplot(aes(x = Group, y = Value)) +
  geom_beeswarm()
```

Linear model
```{r}
m1 <- lm(Value ~ Group, data = data)
summary(m1)
```
```{r}
m1$coefficients
```


Plotting the results of the linear model
```{r}

coeffs_df <- tibble(
  Coeff_id = paste0("b", 0:(length(m1$coefficients)-1)),
  Coeff = m1$coefficients,
  Plot_x = levels(data$Group),
  Plot_y = c(0, m1$coefficients[1], m1$coefficients[1]),
  Plot_yend = c(m1$coefficients[1], 
                m1$coefficients[1] + m1$coefficients[2], 
                m1$coefficients[1] + m1$coefficients[3])
)

data %>% 
  ggplot(aes(x = Group, y = Value)) +
  geom_beeswarm(alpha = 0.3) +
  geom_point(data = coeffs_df, 
             aes(x = Plot_x, y = Plot_yend), 
             color = "red", shape = 18, size = 3) +
  geom_segment(data = coeffs_df, 
               aes(x = Plot_x, xend = Plot_x, y = Plot_y, yend = Plot_yend), 
               color = "red",
               arrow = arrow(length = unit(0.03, "npc"))) +
  geom_hline(yintercept = m1$coefficients[1], color = "red", linetype = "dashed") +
  ylim(0, 6.5)
  
```

```{r}
coded <- model.matrix(~Group, data = data)
print(coded)
```

```{r}
rafalib::imagemat(coded, main ="Model matrix")
```

Let's look at the linear model again:
```{r}
summary(m1)
```
```{r}

anova_result <- anova(m1)
str(anova_result)
```


**Interpretation:** Comparing between the two models: 

* Null model (\\(H_0\\)): the grand mean adequately represents the data
* Alternative model (\\(H_1\\)): three means adequately represents the data

Assuming that \\((H_0\\)) is true, the data doesn't quite compatible with \\(H_0\\), *F*(`r anova_result$Df[1]`, `r anova_result$Df[2]`) = `r anova_result[["F value"]][1]`, *p* <.001. Therefore, we choose to beleive in \\(H_1\\).

(Exact *p* = = `r anova_result[["Pr(>F)"]][1]`)

We also know the location of each mean from the coefficients. What we do not yet know is whether the three means differs significantly or not.


# Checking the differences: pairwise comparison
We can compare pairs that we are interested in.

If we don't have any plan, we can compare three hypotheses simultaneously:
* \\(H_{0, 1}\\): The difference between mean of Group A and Group B is equal to zero: \\(\\mu_A - \\mu_B = 0\\) 
* \\(H_{0, 2}\\): \\(\\mu_A - \\mu_C = 0\\) 
* \\(H_{0, 3}\\): \\(\\mu_B - \\mu_C = 0\\) 
```{r}
group_pairwise <- glht(m1, linfct = mcp(Group = "Tukey"))
# glht: Generalized Linear Hypothesis Tests
# linfct: LINear FunCTion; mcp: Multiple ComParison

summary(group_pairwise)
```

We can plot the differences out. 
```{r}
group_pairwise_t <- tidy(confint(group_pairwise))
group_pairwise_t %>% 
  ggplot(aes(x = paste(lhs, "==", rhs))) +
  geom_hline(yintercept = 0, color = "red") +     # reference line
  geom_point(aes(y = estimate)) +                  # point estimate of the difference
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = 0) + # 95% CI of the difference
  expand_limits(y = 0) +
  coord_flip() +
  xlab("Hypothesis") +
  ylab("Estimate of the difference")
```

**Interpretation:** In the plot above, we can see that the difference between C vs. B and B vs. A are pretty close to zero. The 95% confidence interval of the C vs. B pair actually crosses zero. Therefore, we'd interpret that the difference between C vs. B is unlikely. 

For B vs. A, although the confidence interval doesn't include zero, it's pretty near. At this point, we should use the domain knowledge in our judgement. For example, if the difference is in terms of task completion time in seconds. The difference around 0.15 - 2.6 second wouldn't be a big deal. However, if the data is the difference between cost in million Euros, the difference between 0.15 -2.6 millions would be more notable.

The interpretation of C vs. A is left as your exercise.


# Checking the differences: planned contrast
But if we have planned our comparison beforehand. For example, if A is the baseline, and B, C are treatments conditions. We want to compare the following pairs

* \\(H_{0, 1}\\): Baseline vs. treatments \\(\\mu_A - \\mu_BC = 0\\) 
* \\(H_{0, 2}\\): Compare between the two treatments \\(\\mu_B - \\mu_C = 0\\) 
```{r}
group_contrast <- glht(m1, 
  linfct = mcp(
    Group = c(
      "A - (B + C) = 0",     # null hypothesis 1
      "B - C = 0"            # null hypothesis 2
)))
print(summary(group_contrast))

# calculate confidence interval for each of the hypotheses
group_contrast_t <- tidy(confint(group_contrast))

group_contrast_t %>% 
  
  # add readable hypothesis column
  mutate(Hypothesis = paste(lhs, "==", rhs)) %>% 
  
  # reverse order to counter the effect of the following coord_flip()
  mutate(Hypothesis = factor(Hypothesis, levels = (rev(Hypothesis)))) %>%
  
  # plot
  ggplot(aes(x = Hypothesis)) +
  geom_hline(yintercept = 0, color = "red") +     # reference line
  geom_point(aes(y = estimate)) +                  # point estimate of the difference
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = 0) + # 95% CI of the difference
  expand_limits(y = 0) +
  coord_flip() +
  xlab("Hypothesis") +
  ylab("Estimate of the difference")
```

In general, good experimental designs have planned contrasts in advance, thus avoid testing too many hypotheses simultaneously.

```{r}
summary(glht(m1, 
  linfct = mcp(
    Group = c(
      "A - (B + C) = 0"# null hypothesis 1
))))
```





